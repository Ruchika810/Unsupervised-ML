{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ZZ_AZ50Vyhku",
        "fRzor1pZzCUj",
        "psNyxDbwzMtc",
        "IIeTy_tBzRjn",
        "ctcxAq-J0JE8",
        "t0H8F9YN8GJo",
        "9LUjXzspZYhP",
        "dD975scfBNDE",
        "yak2eC_MKxrp",
        "H7Q6OH5WGIy_",
        "n3ooEBsXGWqx",
        "F8ZwaDyYHfza",
        "3Tugd8cuIRVS",
        "gBQgUlHrttSF",
        "HmJpIe3wpiBK",
        "s9muqkeIp3h9",
        "MGdSoXrjxLYf",
        "CJ9KiaYkq8hj",
        "xOUHaPep7E1G"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ruchika810/Unsupervised-ML/blob/main/Team_notebook_Zomato_Restaurant_Clustering_and_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**"
      ],
      "metadata": {
        "id": "ZZ_AZ50Vyhku"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zomato is an Indian restaurant aggregator and food delivery start-up founded by Deepinder Goyal and Pankaj Chaddah in 2008. Zomato provides information, menus and user-reviews of restaurants, and also has food delivery options from partner restaurants in select cities.\n",
        "\n",
        "India is quite famous for its diverse multi cuisine available in a large number of restaurants and hotel resorts, which is reminiscent of unity in diversity. Restaurant business in India is always evolving. More Indians are warming up to the idea of eating restaurant food whether by dining outside or getting food delivered. The growing number of restaurants in every state of India has been a motivation to inspect the data to get some insights, interesting facts and figures about the Indian food industry in each city. So, this project focuses on analysing the Zomato restaurant data for each city in India.\n",
        "\n",
        "The Project focuses on Customers and Company, you have  to analyze the sentiments of the reviews given by the customer in the data and made some useful conclusion in the form of Visualizations. Also, cluster the zomato restaurants into different segments. The data is vizualized as it becomes easy to analyse data at instant. The Analysis also solve some of the business cases that can directly help the customers finding the Best restaurant in their locality and for the company to grow up and work on the fields they are currently lagging in.\n",
        "\n",
        "This could help in clustering the restaurants into segments. Also the data has valuable information around cuisine and costing which can be used in cost vs. benefit analysis\n",
        "\n",
        "Data could be used for sentiment analysis. Also the metadata of reviewers can be used for identifying the critics in the industry. "
      ],
      "metadata": {
        "id": "jQkvQ5qJyhCZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Attribute Information**"
      ],
      "metadata": {
        "id": "fRzor1pZzCUj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Zomato Restaurant names and Metadata**\n",
        "Use this dataset for clustering part"
      ],
      "metadata": {
        "id": "Het__DPAzFht"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Name : Name of Restaurants\n",
        "\n",
        "2. Links : URL Links of Restaurants\n",
        "\n",
        "3. Cost : Per person estimated Cost of dining\n",
        "\n",
        "4. Collection : Tagging of Restaurants w.r.t. Zomato categories\n",
        "\n",
        "5. Cuisines : Cuisines served by Restaurants\n",
        "\n",
        "6. Timings : Restaurant Timings"
      ],
      "metadata": {
        "id": "xcWz5_X-zJNX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Zomato Restaurant reviews**\n",
        "Merge this dataset with Names and Matadata and then use for sentiment analysis part"
      ],
      "metadata": {
        "id": "psNyxDbwzMtc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Restaurant : Name of the Restaurant\n",
        "\n",
        "2. Reviewer : Name of the Reviewer\n",
        "\n",
        "3. Review : Review Text\n",
        "\n",
        "4. Rating : Rating Provided by Reviewer\n",
        "\n",
        "5. MetaData : Reviewer Metadata - No. of Reviews and followers\n",
        "\n",
        "6. Time: Date and Time of Review\n",
        "\n",
        "7. Pictures : No. of pictures posted with review"
      ],
      "metadata": {
        "id": "Wh6MGlClzO6A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Importing libraries and reading the datasets**"
      ],
      "metadata": {
        "id": "IIeTy_tBzRjn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9rBheb3yceY"
      },
      "outputs": [],
      "source": [
        "# Importing the libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "import scipy.cluster.hierarchy as sch\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from prettytable import PrettyTable \n",
        "\n",
        "import nltk\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import xgboost as xgb\n",
        "from sklearn import svm\n",
        "\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mounting google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyggMu0wzlU2",
        "outputId": "7a8abd65-6114-4223-ea0e-867994906d35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the datasets\n",
        "df1 = pd.read_csv('/content/drive/MyDrive/Datasets/Zomato Restaurant names and Metadata.csv')\n",
        "df2 = pd.read_csv('/content/drive/MyDrive/Datasets/Zomato Restaurant reviews.csv')"
      ],
      "metadata": {
        "id": "kQGm15LUzyvz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "2e37e745-5a84-424c-a887-19637adbdb2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-a760c8be2d9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Importing the datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Datasets/Zomato Restaurant names and Metadata.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Datasets/Zomato Restaurant reviews.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Datasets/Zomato Restaurant names and Metadata.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Inspecting the \"Zomato Restaurant names and Metadata\" dataset**"
      ],
      "metadata": {
        "id": "ctcxAq-J0JE8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First five rows of the dataset\n",
        "df1.head()"
      ],
      "metadata": {
        "id": "1rA4OWmYz27g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Last five rows of the dataset\n",
        "df1.tail()"
      ],
      "metadata": {
        "id": "WGXExxF90wez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shape of the dataset\n",
        "df1.shape"
      ],
      "metadata": {
        "id": "p6ifr7S904xQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset consists of the data of 105 restaurants, which is represented by 6 columns including the name of the restaurant."
      ],
      "metadata": {
        "id": "Nf-eMSC71GSC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data type of each column\n",
        "df1.info()"
      ],
      "metadata": {
        "id": "ffHg5c2_1kHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here the cost column needs to be of datatype int or float."
      ],
      "metadata": {
        "id": "m5K5Qppy13-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Changing data type of cost column from object to integer\n",
        "df1['Cost'] = df1['Cost'].str.replace(\",\" , \"\").astype('int64')"
      ],
      "metadata": {
        "id": "nRu2Ko_m2FdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# finding statistical measures of numerical column\n",
        "df1.describe()"
      ],
      "metadata": {
        "id": "EmNPaO5o2ce3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking skewness of cost column\n",
        "df1.skew()"
      ],
      "metadata": {
        "id": "cJ3KsHPx31wx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here the distribution of cost column is positively skewed. It can also be visualized using dist plot."
      ],
      "metadata": {
        "id": "nz_5B7XV4AmZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating dist plot of cost column\n",
        "sns.distplot(df1['Cost'], hist = False)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-gEUh8314629"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking null value count of each column\n",
        "df1.isnull().sum()"
      ],
      "metadata": {
        "id": "TluF7SpL5stW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are null values in \"Collections\" and \"Timings\" columns. As there columns are of type object we can replace these null values with a string."
      ],
      "metadata": {
        "id": "bVsXUVXt7Okt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filling null values with 'Unknown'\n",
        "df1.fillna('Unknown', inplace = True)"
      ],
      "metadata": {
        "id": "mSFC-WQ76o3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for any dulicate rows\n",
        "df1[df1.duplicated()].sum()"
      ],
      "metadata": {
        "id": "0GLz_nE57idI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are no duplicate rows. "
      ],
      "metadata": {
        "id": "UZVS7xB87sCk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now we can proceed towards the exploratory data analysis part where we will find some insights from the dataset.**"
      ],
      "metadata": {
        "id": "UYSs_0cQ7wPi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EDA on \"Zomato Restaurant names and Metadata\" dataset"
      ],
      "metadata": {
        "id": "t0H8F9YN8GJo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cuisine_list = df1['Cuisines'].str.split(', ')         # Separating all the cuisines by spliting the column by comma.\n",
        "restaurants = {}                                       # Creating an empty dictionary which will store the cuisine name as key and count of restaurant as value\n",
        "for i in cuisine_list:                                 # Iterating through each index\n",
        "  for j in i:                                          # Iterating inside a particular index\n",
        "    if (j in restaurants):\n",
        "      restaurants[j] += 1\n",
        "    else:\n",
        "      restaurants[j] = 1"
      ],
      "metadata": {
        "id": "ogzVwuay8Fj4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = pd.DataFrame(restaurants.values(),index = restaurants.keys(), columns = {'Number_of_Restaurants'})  # Converting the above dictionary to dataframe\n",
        "X.sort_values(by = 'Number_of_Restaurants',ascending = False,inplace = True)                 # Sorting the df by descending order to get most available cusines at top\n",
        "X = X.head(10)  # fetching the top 10 cuisines"
      ],
      "metadata": {
        "id": "IZN6hXpJ8epA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the above result\n",
        "plt.figure(figsize = (14, 6))\n",
        "sns.barplot(x = 'Number_of_Restaurants', y = X.index,  data = X, palette = \"mako\")\n",
        "plt.title(\"Top 10 popular cuisines\", size = 25)\n",
        "plt.xlabel(\"Number of Restaurants\", size = 15)\n",
        "plt.ylabel(\"Cuisines\", size = 15)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZbquZE2i8hXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"North Indian\" cuisine is the most popular cuisine which is available in more than 50% of restaurants.\n",
        "\n",
        "\"Chinese\" cuisine is the 2nd most available cuisine."
      ],
      "metadata": {
        "id": "uLub7UUc-Z3F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "collection_list = df1['Collections'].str.split(', ')  # Separating all the cuisines by spliting the column by comma.\n",
        "rest = {}                                             # Creating an empty dictionary which will store the collection name as key and count of restaurant as value\n",
        "for i in collection_list:                             # Iterating through each index\n",
        "  for j in i:                                         # Iterating inside a particular index\n",
        "    if (j in rest):\n",
        "      rest[j] += 1\n",
        "    else:\n",
        "      rest[j] = 1"
      ],
      "metadata": {
        "id": "yGe0jgGg_fw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y = pd.DataFrame(rest.values(),index = rest.keys(), columns = {'Number_of_Restaurants'})  # Converting the above dictionary to dataframe\n",
        "Y.sort_values(by = 'Number_of_Restaurants',ascending = False,inplace = True)     # Sorting the df by descending order to get most available collection at top\n",
        "Y = Y[1:11]"
      ],
      "metadata": {
        "id": "o4ZSHfJp_uFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the above result\n",
        "plt.figure(figsize = (14, 6))\n",
        "sns.barplot(x = 'Number_of_Restaurants', y = Y.index,  data = Y, palette = \"rocket\")\n",
        "plt.title(\"Top 10 popular collections\", size = 25)\n",
        "plt.xlabel(\"Number of Restaurants\", size = 15)\n",
        "plt.ylabel(\"Collections\", size = 15)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_ca3kxtm_zCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a new dataframe which is sorted by cost.\n",
        "rest_cost = df1.sort_values(by = 'Cost',ascending = False)"
      ],
      "metadata": {
        "id": "WbDqi7sUBmim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Top 10 most expensive restaurants\n",
        "rest_cost[['Name','Cuisines','Cost']][0:10]"
      ],
      "metadata": {
        "id": "JG7Bie2JBrxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Top 10 cheapest restaurants\n",
        "rest_cost[['Name','Cuisines','Cost']].tail(10).sort_values(by = 'Cost', ascending = True)"
      ],
      "metadata": {
        "id": "XKj9ce-4Bx5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Inspecting the \"Zomato Restaurant reviews\" dataset**"
      ],
      "metadata": {
        "id": "9LUjXzspZYhP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First five rows of the dataset\n",
        "df2.head()"
      ],
      "metadata": {
        "id": "n9yh-q25ZcOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Last five rows of the dataset\n",
        "df2.tail()"
      ],
      "metadata": {
        "id": "ib_ej9B_aNdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shape of the dataset\n",
        "df2.shape"
      ],
      "metadata": {
        "id": "99-EkRJkagx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are total 10000 reviews for the 105 restaurants."
      ],
      "metadata": {
        "id": "RsEz17WOanmy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking null value count of each column\n",
        "df2.isnull().sum()"
      ],
      "metadata": {
        "id": "8WlEVv8_b5WY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are few null value in each column. Instead of filling those few null values, it's better to drop those rows."
      ],
      "metadata": {
        "id": "RNEzY-2BcRjj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping the null values\n",
        "df2.dropna(inplace = True)"
      ],
      "metadata": {
        "id": "tio2L-hichCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# column information\n",
        "df2.info()"
      ],
      "metadata": {
        "id": "MrqzHHBca0uP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we need to change the dtype of rating column from object to int/float. "
      ],
      "metadata": {
        "id": "mmDvHV0TctBq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the values of rating column\n",
        "df2['Rating'].value_counts()"
      ],
      "metadata": {
        "id": "Z7mtXmtEdAZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As there is a word 'Like' in the rating column, we can't convert the rating column dtype to integer/float. So to proceed further we have to remove/replace this word from the column."
      ],
      "metadata": {
        "id": "ugdAuk_-dPm3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replacing 'Like' word\n",
        "df2['Rating'] = df2['Rating'].replace('Like', 5)"
      ],
      "metadata": {
        "id": "K0f6M4hucsLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting dtype of 'Rating' from object to float64. \n",
        "df2['Rating'] = df2['Rating'].astype('float64')"
      ],
      "metadata": {
        "id": "8wYc0nYndsP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now from the metadata column, we need to separate the reviews and followers. For this we will make 2 separate column to store those values."
      ],
      "metadata": {
        "id": "giFhy_l5d5pd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df2['No_of_Reviews'],df2['No_of_Followers']=df2['Metadata'].str.split(',').str        # Splitting by comma.\n",
        "df2['No_of_Reviews'] = pd.to_numeric(df2['No_of_Reviews'].str.split(' ').str[0])      # Splitting by space and fetching the zeroth index\n",
        "df2['No_of_Followers'] = pd.to_numeric(df2['No_of_Followers'].str.split(' ').str[1])  # Splitting by space and fetching the first index\n",
        "\n",
        "# Removing the 'Metadata' column\n",
        "df2.drop(['Metadata'], axis = 1, inplace=True)"
      ],
      "metadata": {
        "id": "_f4aIJEwd5S_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the final modified dataset\n",
        "df2.head()"
      ],
      "metadata": {
        "id": "-qZmx3mSe1pP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.info()"
      ],
      "metadata": {
        "id": "hEHEuBA3A7yJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now we can proceed towards the exploratory data analysis part where we will find some insights from the dataset.**"
      ],
      "metadata": {
        "id": "W2KNuzX2fHyF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EDA on \"Zomato Restaurant reviews\" dataset"
      ],
      "metadata": {
        "id": "eQvUUbcyfIVn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Top rated restaurants\n",
        "plt.figure(figsize=(10,6))\n",
        "df2.groupby('Restaurant')['Rating'].mean().sort_values(ascending = False).head(10).plot.barh(color = 'g') # Finding the average rating of each restaurant.\n",
        "plt.title(\"Top rated restaurants\", fontsize=20)\n",
        "plt.xlabel(\"Average Rating\", fontsize=15)\n",
        "plt.ylabel(\"Name of the restaurant\", fontsize=15)\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lgUnNCNPfJdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Worst rated restaurants\n",
        "plt.figure(figsize=(10,6))\n",
        "df2.groupby('Restaurant')['Rating'].mean().sort_values().head(10).plot.barh(color = 'darkred')\n",
        "plt.title(\"Worst rated restaurants\", fontsize=20)\n",
        "plt.xlabel(\"Average Rating\", fontsize=15)\n",
        "plt.ylabel(\"Name of the restaurant\", fontsize=15)\n",
        "plt.xlim([0, 5])\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "w0M7LMJ2f7X2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Top reviewers\n",
        "plt.figure(figsize=(12,6))\n",
        "df2['Reviewer'].value_counts().sort_values(ascending = False).head(10).plot.bar(color = 'b')\n",
        "plt.title(\"Top reviewers\", fontsize=25)\n",
        "plt.xlabel(\"Reviewer\", fontsize=15)\n",
        "plt.ylabel(\"Number of reviews\", fontsize=15)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wiufansRgCBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Most reviewed restaurants\n",
        "plt.figure(figsize=(10,6))\n",
        "df2.groupby('Restaurant')['No_of_Reviews'].sum().sort_values(ascending = False).head(10).plot.barh(color = 'mediumspringgreen')\n",
        "plt.title(\"Most reviewed restaurants\", fontsize=20)\n",
        "plt.xlabel(\"Number of reviews\", fontsize=15)\n",
        "plt.ylabel(\"Name of the restaurant\", fontsize=15)\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "y1O4JC9ehQxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Most followed restaurants\n",
        "plt.figure(figsize=(10,6))\n",
        "df2.groupby('Restaurant')['No_of_Followers'].sum().sort_values(ascending = False).head(10).plot.barh(color = 'indigo')\n",
        "plt.title(\"Most followed restaurants\", fontsize=20)\n",
        "plt.xlabel(\"Number of followers\", fontsize=15)\n",
        "plt.ylabel(\"Name of the restaurant\", fontsize=15)\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "f2p3xQ3yhrb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data preparation for clustering**"
      ],
      "metadata": {
        "id": "dD975scfBNDE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Changing the column name which will help to merge both the datasets.\n",
        "df2.rename(columns = {'Restaurant':'Name'}, inplace = True)"
      ],
      "metadata": {
        "id": "VRCtI2G_BX6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to aggregate the 'Rating', 'No_of_Reviews' and 'No_of_Followers' to make it a single value for each restaurant. For this we will take average of each column by grouping them restaurant name wise."
      ],
      "metadata": {
        "id": "hr527yOlEBAX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "restaurants = list(df2['Name'].unique())\n",
        "# Initializing three new columns\n",
        "df2['Mean_Rating'] = 0        \n",
        "df2['Mean_Reviews'] = 0\n",
        "df2['Mean_Followers'] = 0\n",
        "\n",
        "for i in range(len(restaurants)):\n",
        "    df2['Mean_Rating'][df2['Name'] == restaurants[i]] = df2['Rating'][df2['Name'] == restaurants[i]].mean()\n",
        "    df2['Mean_Reviews'][df2['Name'] == restaurants[i]] = df2['No_of_Reviews'][df2['Name'] == restaurants[i]].mean()\n",
        "    df2['Mean_Followers'][df2['Name'] == restaurants[i]] = df2['No_of_Followers'][df2['Name'] == restaurants[i]].mean()"
      ],
      "metadata": {
        "id": "eoM1y4H-DUiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.head()"
      ],
      "metadata": {
        "id": "2GTSQXyNFVLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ceaating a new dataframe by taking only the required columns for clustering\n",
        "df_clust2 = df2[['Name', 'Mean_Rating',\t'Mean_Followers']]"
      ],
      "metadata": {
        "id": "JuMYuUb0FaiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As there will be duplicate value in the newly formed dataset, we have to remove them."
      ],
      "metadata": {
        "id": "K2KeOCDDF8N6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_clust2.drop_duplicates(inplace = True)"
      ],
      "metadata": {
        "id": "RewulJStGGu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separating out the required columns from df1 for clustering\n",
        "df_clust1 = df1[['Name', 'Cost']]"
      ],
      "metadata": {
        "id": "3zl2M_DnGOi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merging both the datasets\n",
        "final_df = pd.merge(df_clust1, df_clust2, on = 'Name')"
      ],
      "metadata": {
        "id": "0wCcpBmdGayv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df.head(10)"
      ],
      "metadata": {
        "id": "ofnmWDUVGrEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df.describe()"
      ],
      "metadata": {
        "id": "K7FnVJRMG8Yb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the distribution of all the columns\n",
        "for var in final_df.describe().columns:\n",
        "  sns.distplot(final_df[var].dropna())\n",
        "  plt.ylabel('frequency')\n",
        "  plt.xlabel(var)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "JsMg-dkoHBjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the distribution of 'Cost' and 'Mean_Followers' are slightly right skewed, we will apply square root transformation on these columns to make it normally distributed."
      ],
      "metadata": {
        "id": "U7tJfRjlIcwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying square root transformation on 'Cost' and 'Mean_Followers' column\n",
        "final_df['Cost'] = final_df['Cost']**0.5\n",
        "final_df['Mean_Followers'] = final_df['Mean_Followers']**0.5"
      ],
      "metadata": {
        "id": "yTsyv6t7HQ-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the distribution of all the columns\n",
        "for var in final_df.describe().columns:\n",
        "  sns.distplot(final_df[var].dropna())\n",
        "  plt.ylabel('frequency')\n",
        "  plt.xlabel(var)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "g_6x3fZUJC-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now as all the features are nearly normally distributed we can proceed futher to cluster them together."
      ],
      "metadata": {
        "id": "KLQmrczsRWXu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Clustering**"
      ],
      "metadata": {
        "id": "yak2eC_MKxrp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clustering by 'Cost' and 'Mean_Rating' "
      ],
      "metadata": {
        "id": "H7Q6OH5WGIy_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature scaling**"
      ],
      "metadata": {
        "id": "zN6kyHaTbjz-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Scaling is a technique of bringing down the values of all the independent features of the dataset on the same scale. Feature selection helps to do calculations in algorithms very quickly. It is the important stage of data preprocessing."
      ],
      "metadata": {
        "id": "PtCmnFUNeqw1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features_rec_mon=['Cost', 'Mean_Rating']\n",
        "X_features_rec_mon = final_df[features_rec_mon].values\n",
        "scaler_rec_mon = preprocessing.StandardScaler()\n",
        "X_rec_mon=scaler_rec_mon.fit_transform(X_features_rec_mon)\n",
        "X=X_rec_mon"
      ],
      "metadata": {
        "id": "6Ce1I6WTSaCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Silhouette score**"
      ],
      "metadata": {
        "id": "57v_T6yfZ2Jg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Silhouette score is used to evaluate the quality of clusters created using clustering algorithms such as K-Means in terms of how well samples are clustered with other samples that are similar to each other. The Silhouette score is calculated for each sample of different clusters. The more is the Silhouette score, better the clusters are."
      ],
      "metadata": {
        "id": "VCZuxViGRHxw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating silhouette score for a range of clusters\n",
        "range_n_clusters = [2,3,4,5,6,7,8,9]\n",
        "for n_clusters in range_n_clusters:\n",
        "    clusterer = KMeans(n_clusters=n_clusters, random_state = 100)\n",
        "    preds = clusterer.fit_predict(X)\n",
        "    centers = clusterer.cluster_centers_\n",
        "\n",
        "    score = silhouette_score(X, preds)\n",
        "    print(\"For n_clusters = {}, silhouette score is {}\".format(n_clusters, score))"
      ],
      "metadata": {
        "id": "2MQGjhMDK11d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Optimal number of cluster = 4**"
      ],
      "metadata": {
        "id": "xQteZZRWF1Dh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Elbow method**"
      ],
      "metadata": {
        "id": "gG2gmvReZ8fU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In cluster analysis, the elbow method is a heuristic used in determining the number of clusters in a data set. The method consists of plotting the explained variation as a function of the number of clusters, and picking the elbow of the curve as the number of clusters to use."
      ],
      "metadata": {
        "id": "F6Ae3jN_R2zZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Elbow method\n",
        "sum_of_sq_dist = {}\n",
        "for k in range(1,15):\n",
        "    km = KMeans(n_clusters= k, init= 'k-means++', max_iter= 1000, random_state = 100)\n",
        "    km = km.fit(X)\n",
        "    sum_of_sq_dist[k] = km.inertia_\n",
        "    \n",
        "#Plot the graph for the sum of square distance values and Number of Clusters\n",
        "sns.pointplot(x = list(sum_of_sq_dist.keys()), y = list(sum_of_sq_dist.values()))\n",
        "plt.xlabel('Number of Clusters(k)')\n",
        "plt.ylabel('Sum of Square Distances')\n",
        "plt.title('Elbow Method For Optimal k')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "euSZ3ETZNTad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Optimal number of cluster = 4**"
      ],
      "metadata": {
        "id": "9At87wPWF7ax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting kmeans clustering algorithm\n",
        "kmeans = KMeans(n_clusters=4,random_state = 100)\n",
        "kmeans.fit(X)\n",
        "y_kmeans= kmeans.predict(X)"
      ],
      "metadata": {
        "id": "iNKzblpQNv-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the clusters\n",
        "plt.figure(figsize=(15,10))\n",
        "plt.title('Restaurant segmentation based on Cost and Mean_Rating')\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=100)\n",
        "\n",
        "# Plotting cluster centres\n",
        "centers = kmeans.cluster_centers_\n",
        "plt.scatter(centers[:, 0], centers[:, 1], c='red', s=400, alpha=0.5)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LWHaBFD0N4S5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Finding the clusters for the observation given in the dataset\n",
        "final_df['Cluster'] = kmeans.labels_\n",
        "final_df[['Name', 'Cost', 'Mean_Rating','Cluster']].head(10)"
      ],
      "metadata": {
        "id": "yj5K8ywVN7Z-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dendrogram**"
      ],
      "metadata": {
        "id": "vSD-pSsacU8n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The sole concept of hierarchical clustering lies in just the construction and analysis of a dendrogram. A dendrogram is a tree-like structure that explains the relationship between all the data points in the system."
      ],
      "metadata": {
        "id": "vondhhMvfRZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the dendrogram to find the optimal number of clusters\n",
        "plt.figure(figsize=(13,8))\n",
        "dendrogram = sch.dendrogram(sch.linkage(X, method = 'ward'))\n",
        "plt.title('Dendrogram')\n",
        "plt.xlabel('Restaurants')\n",
        "plt.ylabel('Euclidean Distances')\n",
        "plt.axhline(y=10, color='r', linestyle='--')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "P25SJ0AnPzu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Optimal number of cluster = 2**"
      ],
      "metadata": {
        "id": "zhIvEUCeGAOj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting hierarchical clustering\n",
        "hc = AgglomerativeClustering(n_clusters = 2, affinity = 'euclidean', linkage = 'ward')\n",
        "y_hc = hc.fit_predict(X)"
      ],
      "metadata": {
        "id": "jSB89mk6QZjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the clusters (two dimensions only)\n",
        "plt.figure(figsize=(13,8))\n",
        "plt.scatter(X[y_hc == 0, 0], X[y_hc == 0, 1], s = 100, c = 'red', label = 'Category 1')\n",
        "plt.scatter(X[y_hc == 1, 0], X[y_hc == 1, 1], s = 100, c = 'blue', label = 'Category 2')\n",
        "\n",
        "plt.title('Clusters of Restaurants')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5c8I2siZQjA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clustering by 'Mean_Rating' and 'Mean_Followers' "
      ],
      "metadata": {
        "id": "n3ooEBsXGWqx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature scaling**"
      ],
      "metadata": {
        "id": "Mae8sqMubrPT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features_rec_mon=['Mean_Rating', 'Mean_Followers']\n",
        "X_features_rec_mon = final_df[features_rec_mon].values\n",
        "scaler_rec_mon = preprocessing.StandardScaler()\n",
        "X_rec_mon=scaler_rec_mon.fit_transform(X_features_rec_mon)\n",
        "X=X_rec_mon"
      ],
      "metadata": {
        "id": "PpZdrKoWTI-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Silhouette score**"
      ],
      "metadata": {
        "id": "r6rHubDAbxIz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating silhouette score for a range of clusters\n",
        "range_n_clusters = [2,3,4,5,6,7,8,9]\n",
        "for n_clusters in range_n_clusters:\n",
        "    clusterer = KMeans(n_clusters=n_clusters, random_state = 100)\n",
        "    preds = clusterer.fit_predict(X)\n",
        "    centers = clusterer.cluster_centers_\n",
        "\n",
        "    score = silhouette_score(X, preds)\n",
        "    print(\"For n_clusters = {}, silhouette score is {}\".format(n_clusters, score))"
      ],
      "metadata": {
        "id": "Do8b3LtfGWqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Optimal number of cluster = 3**"
      ],
      "metadata": {
        "id": "NHiULFOnGWqy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Elbow method**"
      ],
      "metadata": {
        "id": "FtI8oIBXb2LI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sum_of_sq_dist = {}\n",
        "for k in range(1,15):\n",
        "    km = KMeans(n_clusters= k, init= 'k-means++', max_iter= 1000, random_state = 100)\n",
        "    km = km.fit(X)\n",
        "    sum_of_sq_dist[k] = km.inertia_\n",
        "    \n",
        "#Plotting the graph for the sum of square distance values and Number of Clusters\n",
        "sns.pointplot(x = list(sum_of_sq_dist.keys()), y = list(sum_of_sq_dist.values()))\n",
        "plt.xlabel('Number of Clusters(k)')\n",
        "plt.ylabel('Sum of Square Distances')\n",
        "plt.title('Elbow Method For Optimal k')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xc2jBHz8GWqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Optimal number of cluster = 3**"
      ],
      "metadata": {
        "id": "dfGfLAJfGWqy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting kmeans clustering algothm\n",
        "kmeans = KMeans(n_clusters=3,random_state = 100)\n",
        "kmeans.fit(X)\n",
        "y_kmeans= kmeans.predict(X)"
      ],
      "metadata": {
        "id": "zP_ptFSQGWqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the clusters\n",
        "plt.figure(figsize=(15,10))\n",
        "plt.title('Restaurant segmentation based on Mean_Rating and Mean_Followers')\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=100)\n",
        "\n",
        "# Plotting cluster centres\n",
        "centers = kmeans.cluster_centers_\n",
        "plt.scatter(centers[:, 0], centers[:, 1], c='red', s=400, alpha=0.5)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qN4_uWRMGWqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Finding the clusters for the observation given in the dataset\n",
        "final_df['Cluster'] = kmeans.labels_\n",
        "final_df[['Name', 'Mean_Rating', 'Mean_Followers','Cluster']].head(10)"
      ],
      "metadata": {
        "id": "OQPn13O6Qgu7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dendrogram**"
      ],
      "metadata": {
        "id": "kMgbo7-8b7wV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the dendrogram to find the optimal number of clusters\n",
        "plt.figure(figsize=(13,8))\n",
        "dendrogram = sch.dendrogram(sch.linkage(X, method = 'ward'))\n",
        "plt.title('Dendrogram')\n",
        "plt.xlabel('Restaurants')\n",
        "plt.ylabel('Euclidean Distances')\n",
        "plt.axhline(y=8.5, color='r', linestyle='--')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MNleU01VGWqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Optimal number of cluster = 3**"
      ],
      "metadata": {
        "id": "hRM2nrGZGWq0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting hierarchical clustering\n",
        "hc = AgglomerativeClustering(n_clusters = 3, affinity = 'euclidean', linkage = 'ward')\n",
        "y_hc = hc.fit_predict(X)"
      ],
      "metadata": {
        "id": "Gd6UFhs4GWq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the clusters (two dimensions only)\n",
        "plt.figure(figsize=(13,8))\n",
        "plt.scatter(X[y_hc == 0, 0], X[y_hc == 0, 1], s = 100, c = 'red', label = 'Category 1')\n",
        "plt.scatter(X[y_hc == 1, 0], X[y_hc == 1, 1], s = 100, c = 'blue', label = 'Category 2')\n",
        "plt.scatter(X[y_hc == 2, 0], X[y_hc == 2, 1], s = 100, c = 'green', label = 'Category 3')\n",
        "\n",
        "plt.title('Clusters of Restaurants')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Vz2bz-6hGWq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clustering by 'Cost' and 'Mean_Followers' "
      ],
      "metadata": {
        "id": "F8ZwaDyYHfza"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature scaling**"
      ],
      "metadata": {
        "id": "ZaOEpSIadVrB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features_rec_mon=['Cost', 'Mean_Followers']\n",
        "X_features_rec_mon = final_df[features_rec_mon].values\n",
        "scaler_rec_mon = preprocessing.StandardScaler()\n",
        "X_rec_mon=scaler_rec_mon.fit_transform(X_features_rec_mon)\n",
        "X=X_rec_mon"
      ],
      "metadata": {
        "id": "jFMrpRIFTVws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Silhouette score**"
      ],
      "metadata": {
        "id": "fKYsP0hqdacs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating silhouette score for a range of clusters\n",
        "range_n_clusters = [2,3,4,5,6,7,8,9]\n",
        "for n_clusters in range_n_clusters:\n",
        "    clusterer = KMeans(n_clusters=n_clusters, random_state = 100)\n",
        "    preds = clusterer.fit_predict(X)\n",
        "    centers = clusterer.cluster_centers_\n",
        "\n",
        "    score = silhouette_score(X, preds)\n",
        "    print(\"For n_clusters = {}, silhouette score is {}\".format(n_clusters, score))"
      ],
      "metadata": {
        "id": "mfUfyQKQHfzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Optimal number of cluster = 9**"
      ],
      "metadata": {
        "id": "ve1WRG9lHfzk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Elbow method**"
      ],
      "metadata": {
        "id": "SKN8PDpRdj-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sum_of_sq_dist = {}\n",
        "for k in range(1,15):\n",
        "    km = KMeans(n_clusters= k, init= 'k-means++', max_iter= 1000, random_state = 100)\n",
        "    km = km.fit(X)\n",
        "    sum_of_sq_dist[k] = km.inertia_\n",
        "    \n",
        "#Plotting the graph for the sum of square distance values and Number of Clusters\n",
        "sns.pointplot(x = list(sum_of_sq_dist.keys()), y = list(sum_of_sq_dist.values()))\n",
        "plt.xlabel('Number of Clusters(k)')\n",
        "plt.ylabel('Sum of Square Distances')\n",
        "plt.title('Elbow Method For Optimal k')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vIHDfIjLHfzk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Optimal number of cluster = 3**"
      ],
      "metadata": {
        "id": "rKTvvQujHfzl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting the kmeans clustering algorithm\n",
        "kmeans = KMeans(n_clusters=3,random_state = 100)\n",
        "kmeans.fit(X)\n",
        "y_kmeans= kmeans.predict(X)"
      ],
      "metadata": {
        "id": "u9pgqvKXHfzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the clusters\n",
        "plt.figure(figsize=(15,10))\n",
        "plt.title('Restaurant segmentation based on Cost and Mean_Followers')\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=100)\n",
        "\n",
        "# Plotting cluster centres\n",
        "centers = kmeans.cluster_centers_\n",
        "plt.scatter(centers[:, 0], centers[:, 1], c='red', s=400, alpha=0.5)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dyPk_bWdHfzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Finding the clusters for the observation given in the dataset\n",
        "final_df['Cluster'] = kmeans.labels_\n",
        "final_df[['Name', 'Cost', 'Mean_Followers','Cluster']].head(10)"
      ],
      "metadata": {
        "id": "1auBpLS-Qjg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dendrogram**"
      ],
      "metadata": {
        "id": "nHykNjrpd0PC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the dendrogram to find the optimal number of clusters\n",
        "plt.figure(figsize=(13,8))\n",
        "dendrogram = sch.dendrogram(sch.linkage(X, method = 'ward'))\n",
        "plt.title('Dendrogram')\n",
        "plt.xlabel('Restaurants')\n",
        "plt.ylabel('Euclidean Distances')\n",
        "plt.axhline(y=10, color='r', linestyle='--')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pDJzCPy9Hfzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Optimal number of cluster = 2**"
      ],
      "metadata": {
        "id": "bdaTOqooHfzm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting hierarchical clustering\n",
        "hc = AgglomerativeClustering(n_clusters = 2, affinity = 'euclidean', linkage = 'ward')\n",
        "y_hc = hc.fit_predict(X)"
      ],
      "metadata": {
        "id": "da9jfqzsHfzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the clusters (two dimensions only)\n",
        "plt.figure(figsize=(13,8))\n",
        "plt.scatter(X[y_hc == 0, 0], X[y_hc == 0, 1], s = 100, c = 'red', label = 'Category 1')\n",
        "plt.scatter(X[y_hc == 1, 0], X[y_hc == 1, 1], s = 100, c = 'blue', label = 'Category 2')\n",
        "\n",
        "plt.title('Clusters of Restaurants')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_VHckFDaHfzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clustering by 'Cost', 'Mean_Rating' and 'Mean_Followers' "
      ],
      "metadata": {
        "id": "3Tugd8cuIRVS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature scaling**"
      ],
      "metadata": {
        "id": "OE9PtbNwd9Ja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features_rec_mon=['Cost', 'Mean_Rating', 'Mean_Followers']\n",
        "X_features_rec_mon = final_df[features_rec_mon].values\n",
        "scaler_rec_mon = preprocessing.StandardScaler()\n",
        "X_rec_mon=scaler_rec_mon.fit_transform(X_features_rec_mon)\n",
        "X=X_rec_mon"
      ],
      "metadata": {
        "id": "bXmSNVLsTgEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Silhouette score**"
      ],
      "metadata": {
        "id": "wJDXWE-Sd_30"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating silhouette score for a range of clusters\n",
        "range_n_clusters = [2,3,4,5,6,7,8,9]\n",
        "for n_clusters in range_n_clusters:\n",
        "    clusterer = KMeans(n_clusters=n_clusters, random_state = 100)\n",
        "    preds = clusterer.fit_predict(X)\n",
        "    centers = clusterer.cluster_centers_\n",
        "\n",
        "    score = silhouette_score(X, preds)\n",
        "    print(\"For n_clusters = {}, silhouette score is {}\".format(n_clusters, score))"
      ],
      "metadata": {
        "id": "L6rMe1U3IRVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Optimal number of cluster = 6**"
      ],
      "metadata": {
        "id": "Ut5Z993sIRVe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Elbow method**"
      ],
      "metadata": {
        "id": "KGqrgwK8eE0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sum_of_sq_dist = {}\n",
        "for k in range(1,15):\n",
        "    km = KMeans(n_clusters= k, init= 'k-means++', max_iter= 1000, random_state = 100)\n",
        "    km = km.fit(X)\n",
        "    sum_of_sq_dist[k] = km.inertia_\n",
        "    \n",
        "#Plotting the graph for the sum of square distance values and Number of Clusters\n",
        "sns.pointplot(x = list(sum_of_sq_dist.keys()), y = list(sum_of_sq_dist.values()))\n",
        "plt.xlabel('Number of Clusters(k)')\n",
        "plt.ylabel('Sum of Square Distances')\n",
        "plt.title('Elbow Method For Optimal k')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "J3p9qhQ-IRVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Optimal number of cluster = 4**"
      ],
      "metadata": {
        "id": "9AwfGQVdIRVf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting the kmeans clustering algorithm\n",
        "kmeans = KMeans(n_clusters=4,random_state = 100)\n",
        "kmeans.fit(X)\n",
        "y_kmeans= kmeans.predict(X)"
      ],
      "metadata": {
        "id": "sdpVKKOtIRVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting the clusters\n",
        "plt.figure(figsize=(15,10))\n",
        "plt.title('Restaurant segmentation based on Cost, Mean_Rating and Mean_Followers')\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=100)\n",
        "\n",
        "# Plotting cluster centres\n",
        "centers = kmeans.cluster_centers_\n",
        "plt.scatter(centers[:, 0], centers[:, 1], c='red', s=400, alpha=0.5)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "F3xSFdERIRVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Finding the clusters for the observation given in the dataset\n",
        "final_df['Cluster'] = kmeans.labels_\n",
        "final_df.head(10)"
      ],
      "metadata": {
        "id": "7c9rQCSXQm1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dendrogram**"
      ],
      "metadata": {
        "id": "h7PPlxrjeXvs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the dendogram to find the optimal number of clusters\n",
        "plt.figure(figsize=(13,8))\n",
        "dendrogram = sch.dendrogram(sch.linkage(X, method = 'ward'))\n",
        "plt.title('Dendrogram')\n",
        "plt.xlabel('Restaurants')\n",
        "plt.ylabel('Euclidean Distances')\n",
        "plt.axhline(y=11, color='r', linestyle='--')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bEyaW37AIRVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Optimal number of cluster = 2**"
      ],
      "metadata": {
        "id": "5UUTQSM3IRVg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting hierarchical clustering\n",
        "hc = AgglomerativeClustering(n_clusters = 2, affinity = 'euclidean', linkage = 'ward')\n",
        "y_hc = hc.fit_predict(X)"
      ],
      "metadata": {
        "id": "O-8wI-WvIRVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the clusters (two dimensions only)\n",
        "plt.figure(figsize=(13,8))\n",
        "plt.scatter(X[y_hc == 0, 0], X[y_hc == 0, 1], s = 100, c = 'red', label = 'Category 1')\n",
        "plt.scatter(X[y_hc == 1, 0], X[y_hc == 1, 1], s = 100, c = 'blue', label = 'Category 2')\n",
        "\n",
        "plt.title('Clusters of Restaurants')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eNYzeO8fIRVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary"
      ],
      "metadata": {
        "id": "8NnNs9e2CgiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the Column Names while initializing the Table \n",
        "myTable = PrettyTable(['SL No.',\"Model_Name\",'Data', \"Optimal_Number_of_clusters\"]) \n",
        "  \n",
        "# Add rows \n",
        "myTable.add_row(['1', \"K-Means with silhouette_score \", \"Cost and Mean_Rating\", \"4\"]) \n",
        "myTable.add_row(['2', \"K-Means with Elbow method  \", \" Cost and Mean_Rating\", \"4\"])\n",
        "myTable.add_row(['3', \"Hirarchical Clustering\", \" Cost and Mean_Rating\", \"2\"]) \n",
        "\n",
        "myTable.add_row(['4',\"K-Means with silhouette_score \", \"Mean_Rating and Mean_Followers\", \"3\"]) \n",
        "myTable.add_row(['5',\"K-Means with Elbow method  \", \"Mean_Rating and Mean_Followers\", \"3\"])\n",
        "myTable.add_row(['6',\"Hierarchical Clustering\", \"Mean_Rating and Mean_Followers\", \"3\"])\n",
        "\n",
        "myTable.add_row(['7',\"K-Means with silhouette_score \", \"Cost and Mean_Followers\", \"9\"]) \n",
        "myTable.add_row(['8',\"K-Means with Elbow method  \", \"Cost and Mean_Followers\", \"3\"])\n",
        "myTable.add_row(['9',\"Hierarchical clustering  \", \"Cost and Mean_Followers\", \"2\"])\n",
        "\n",
        "myTable.add_row(['10',\"K-Means with silhouette_score \", \"Cost, Mean_Rating and Mean_Followers\", \"6\"]) \n",
        "myTable.add_row(['11',\"K-Means with Elbow method  \", \"Cost, Mean_Rating and Mean_Followers\", \"4\"])\n",
        "myTable.add_row(['12',\"Hierarchical clustering  \", \"Cost, Mean_Rating and Mean_Followers\", \"2\"])\n",
        "\n",
        "print(myTable)"
      ],
      "metadata": {
        "id": "C3lEPl9WCkJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion from clustering"
      ],
      "metadata": {
        "id": "jZoeJ3bF6sNe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimal number of clusters by taking two variables at a time are either three or four. And optimal number of clusters by taking all variables at a time is four."
      ],
      "metadata": {
        "id": "Knvc6jPg6w9c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Sentiment Analysis**"
      ],
      "metadata": {
        "id": "gBQgUlHrttSF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preprocessing and EDA"
      ],
      "metadata": {
        "id": "d4g08q75K7-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_df = df2[['Name', 'Review', 'Rating']]"
      ],
      "metadata": {
        "id": "HgO9EVxeMD01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_df.head()"
      ],
      "metadata": {
        "id": "UQU9t8RtMbUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_df['Rating'] = np.where(nlp_df['Rating']<4, 0, 1)"
      ],
      "metadata": {
        "id": "vpshxIpbNNSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_df.head(10)"
      ],
      "metadata": {
        "id": "RE6sMD_qjj8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking whether there is class imbalance or not\n",
        "nlp_df['Rating'].value_counts().plot.bar()\n",
        "plt.title(\"Count of positive and negative reviews\", fontsize=20)\n",
        "plt.ylabel(\"Number of reviews\", fontsize=15)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XKx2USMrn_Jb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Restaurants with most number of positive reviews:**"
      ],
      "metadata": {
        "id": "1Re2PIoUsV-a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Restaurant with most number of positive reviews\n",
        "plt.figure(figsize=(10,6))\n",
        "nlp_df[nlp_df['Rating']==1]['Name'].value_counts()[:10].plot.barh(color = 'g')\n",
        "plt.title(\"Restaurants with most number of positive reviews\", fontsize=20)\n",
        "plt.xlabel(\"Number of positive reviews\", fontsize=15)\n",
        "plt.ylabel(\"Name of the restaurant\", fontsize=15)\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "49VaGoFYr-e7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Restaurants with most number of negative reviews:**"
      ],
      "metadata": {
        "id": "Q8SvvhglwGz8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Restaurant with most number of negative reviews\n",
        "plt.figure(figsize=(10,6))\n",
        "nlp_df[nlp_df['Rating']==0]['Name'].value_counts()[:11].plot.barh(color = 'r')\n",
        "plt.title(\"Restaurants with most number of negative reviews\", fontsize=20)\n",
        "plt.xlabel(\"Number of negative reviews\", fontsize=15)\n",
        "plt.ylabel(\"Name of the restaurant\", fontsize=15)\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HZ6_xtA4u9Mt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_df.head()"
      ],
      "metadata": {
        "id": "7hhsNt5wppDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_df[\"Review\"] = nlp_df[\"Review\"].str.lower()"
      ],
      "metadata": {
        "id": "kY_lknS5qLky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Removing stopwords and punctuations"
      ],
      "metadata": {
        "id": "iYC1SAOlwqX0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stop words are available in abundance in any human language. By removing these words, we remove the low-level information from our text in order to give more focus to the important information."
      ],
      "metadata": {
        "id": "3Nl36-bVTX1K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading stopwords\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "VTpJCEVew0Hd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a function to remove stopwords and punctations from 'Review' column\n",
        "\n",
        "def text_process(msg):\n",
        "    nopunc =[char for char in msg if char not in string.punctuation]\n",
        "    nopunc=''.join(nopunc)\n",
        "    return ' '.join([word for word in nopunc.split() if word.lower() not in stopwords.words('english')])"
      ],
      "metadata": {
        "id": "WqGq5QtExhYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying text_process function to \"Review\" column and storing the changes in a new column\n",
        "nlp_df['Filtered_Review'] = nlp_df['Review'].apply(text_process)"
      ],
      "metadata": {
        "id": "msI5wbNoxpjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(nlp_df['Review'][0])\n",
        "print(nlp_df['Filtered_Review'][0])"
      ],
      "metadata": {
        "id": "l3t6pG02yP00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stemming"
      ],
      "metadata": {
        "id": "HmJpIe3wpiBK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming is a method of normalization of words in Natural Language Processing. It is a technique in which a set of words in a sentence are converted into a sequence to shorten its lookup. In this method, the words having the same meaning but have some variations according to the context or sentence are normalized.\n",
        "\n",
        "In another word, there is one root word, but there are many variations of the same words. For example, the root word is eat and its variations are eats, eating, eaten and like so. In the same way, with the help of Stemming in Python, we can find the root word of any variations."
      ],
      "metadata": {
        "id": "Nsr1RsYHSuR5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating an object of stemming function\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "def stemming(text):\n",
        "  '''a function which stems each word in the given text'''\n",
        "  text = [stemmer.stem(word) for word in text.split()]\n",
        "  return \" \".join(text)"
      ],
      "metadata": {
        "id": "BWUSP-UybQU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying stemming function to the column\n",
        "nlp_df['Filtered_Review'] = nlp_df['Filtered_Review'].apply(stemming)"
      ],
      "metadata": {
        "id": "iUa4dNETofu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vectorization"
      ],
      "metadata": {
        "id": "s9muqkeIp3h9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vectorization is jargon for a classic approach of converting input data from its raw format (i.e. text ) into vectors of real numbers which is the format that ML models support.\n",
        "\n",
        "In Machine Learning, vectorization is a step in feature extraction. The idea is to get some distinct features out of the text for the model to train on, by converting text to numerical vectors."
      ],
      "metadata": {
        "id": "lyImaRRxTQgq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating an object of TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(max_features=500)\n",
        "# Vectorizing The column\n",
        "X = vectorizer.fit_transform(nlp_df['Filtered_Review'])"
      ],
      "metadata": {
        "id": "vPowoU60uhZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Name of the features\n",
        "print(vectorizer.get_feature_names())"
      ],
      "metadata": {
        "id": "jZbrlZnDvFlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.toarray().shape"
      ],
      "metadata": {
        "id": "3HoSfrFQvfVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "b830e24a445ee72dd3fb62104f1b41cc0c3b1fb8",
        "id": "MGdSoXrjxLYf"
      },
      "source": [
        "## Train test split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "1220b3cc2df7226c69142385389249f6184431f0",
        "id": "n0kFql08xLYh"
      },
      "source": [
        "review_train,review_test,label_train,label_test = train_test_split(nlp_df['Filtered_Review'], nlp_df['Rating'],test_size=0.20, random_state = 100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMNDcMXLGW-J"
      },
      "source": [
        "review_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "e2c83a25b80617a905e8c15938de7347b0b7c5e5",
        "id": "rg7fczqpxLYf"
      },
      "source": [
        "With reviews represented as vectors, we can finally train our sentiment analysis classifier. Now we will use Naive Bayes Classifier to perform this classification task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EptnJgdGMUS"
      },
      "source": [
        "train_vectorized = vectorizer.transform(review_train)\n",
        "test_vectorized = vectorizer.transform(review_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wuEx9PrvGhRh"
      },
      "source": [
        "train_vectorized"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltHiuyb2Gdxg"
      },
      "source": [
        "train_array= train_vectorized.toarray()\n",
        "test_array = test_vectorized.toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Naive Bayes Classifier"
      ],
      "metadata": {
        "id": "YfR8LojKtHJl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Naive Bayes algorithms are mostly used in sentiment analysis, spam filtering, recommendation systems etc. They are fast and easy to implement but their biggest disadvantage is that the requirement of predictors to be independent. In most of the real life cases, the predictors are dependent, this hinders the performance of the classifier.\n",
        "\n",
        "This is basically used as a baseline model."
      ],
      "metadata": {
        "id": "twDVvu-Y9KgR"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "9ae436fe7af2fca2dc7b0fd25d44567c88aa24b8",
        "id": "HLrQuWVyxLYf"
      },
      "source": [
        "# Instantiating naive bayes classifier\n",
        "nb_clf = GaussianNB()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model\n",
        "nb_clf.fit(train_array,label_train)"
      ],
      "metadata": {
        "id": "Sjrbv1WPPyK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koOOJdf0FTks"
      },
      "source": [
        "# Predictions\n",
        "train_preds = nb_clf.predict(train_array)\n",
        "test_preds = nb_clf.predict(test_array)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXUONUulFoBm"
      },
      "source": [
        "# Print the classification report for train and test\n",
        "print(classification_report(label_train,train_preds))\n",
        "print(classification_report(label_test,test_preds))\n",
        "\n",
        "# Confusion matrix\n",
        "sns.heatmap(confusion_matrix(label_test, test_preds), annot=True, fmt='d')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic regression"
      ],
      "metadata": {
        "id": "-HB935z6oJHf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In statistics, the (binary) logistic model (or logit model) is a statistical model that models the probability of one event (out of two alternatives) taking place by having the log-odds (the logarithm of the odds) for the event be a linear combination of one or more independent variables (\"predictors\")."
      ],
      "metadata": {
        "id": "4aAcjbTl-L8Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiating logistic regression classifier\n",
        "lr_clf = LogisticRegression(max_iter=1000)"
      ],
      "metadata": {
        "id": "j308WioeoMof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model\n",
        "lr_clf.fit(train_array,label_train)"
      ],
      "metadata": {
        "id": "Q9Jr96cyoQLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the predicted classes\n",
        "lr_train_class_preds = lr_clf.predict(train_array)\n",
        "lr_test_class_preds = lr_clf.predict(test_array)"
      ],
      "metadata": {
        "id": "NPwgkPldo7Lc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification report\n",
        "print(classification_report(label_train, lr_train_class_preds))\n",
        "print(classification_report(label_test, lr_test_class_preds))\n",
        "\n",
        "# Confusion matrix\n",
        "sns.heatmap(confusion_matrix(label_test, lr_test_class_preds), annot=True, fmt='d')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UsbqiycdpGMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XGBoost"
      ],
      "metadata": {
        "id": "vlK91z4kp0py"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way."
      ],
      "metadata": {
        "id": "6LL7jYap-d_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiating XGBoost classifier\n",
        "xgb_clf = xgb.XGBClassifier(random_state=20)"
      ],
      "metadata": {
        "id": "IFScVyUhXTKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model\n",
        "xgb_clf.fit(train_array,label_train)"
      ],
      "metadata": {
        "id": "JUJORy1EXn8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the predicted classes\n",
        "xgb_train_preds = xgb_clf.predict(train_array)\n",
        "xgb_test_preds = xgb_clf.predict(test_array)"
      ],
      "metadata": {
        "id": "WUSqU9okbFuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification report\n",
        "print (classification_report(label_train, xgb_train_preds))\n",
        "print (classification_report(label_test, xgb_test_preds))\n",
        "\n",
        "# Confusion matrix\n",
        "sns.heatmap(confusion_matrix(label_test, xgb_test_preds), annot=True, fmt='d')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-30OLvt3sCym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SVM"
      ],
      "metadata": {
        "id": "z0IkPIzEzAEI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal of the SVM algorithm is to create the best line or decision boundary that can segregate n-dimensional space into classes so that we can easily put the new data point in the correct category in the future. This best decision boundary is called a hyperplane.\n",
        "\n",
        "SVM chooses the extreme points/vectors that help in creating the hyperplane. These extreme cases are called as support vectors, and hence algorithm is termed as Support Vector Machine. "
      ],
      "metadata": {
        "id": "RvyHU5Tg-f98"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiating SVM classifier\n",
        "svm_clf = svm.SVC(kernel = 'rbf', C = 0.5)"
      ],
      "metadata": {
        "id": "ooAhd3NOzAXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model \n",
        "svm_clf.fit(train_array, label_train)"
      ],
      "metadata": {
        "id": "cHh1E0BLPGC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the predicted classes\n",
        "svm_train_preds = svm_clf.predict(train_array)\n",
        "svm_test_preds = svm_clf.predict(test_array)"
      ],
      "metadata": {
        "id": "HfUvIDzK0lRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification report\n",
        "print (classification_report(label_train, svm_train_preds))\n",
        "print (classification_report(label_test, svm_test_preds))\n",
        "\n",
        "# Confusion matrix\n",
        "sns.heatmap(confusion_matrix(label_test, svm_test_preds), annot=True, fmt='d')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hUc9omU90udB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary"
      ],
      "metadata": {
        "id": "CJ9KiaYkq8hj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the Column Names while initializing the Table \n",
        "myTable = PrettyTable(['SL No.',\"Model_Name\",'Train accuracy', \"Test accuracy\"]) \n",
        "  \n",
        "# Add rows \n",
        "myTable.add_row(['1',\"Naive-Bayes \", \"0.84\", \"0.83\"]) \n",
        "myTable.add_row(['2',\"Logistic Regression  \", \"0.88\", \"0.86\"])\n",
        "myTable.add_row(['3',\"XGBoost \", \"0.86\", \"0.84\"]) \n",
        "myTable.add_row(['4',\"SVM \", \"0.92\", \"0.87\"]) \n",
        "\n",
        "print(myTable)"
      ],
      "metadata": {
        "id": "GsN-U2srrJqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion from sentiment analysis"
      ],
      "metadata": {
        "id": "xOUHaPep7E1G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic regression gives good accuracy without overfitting the data.\n",
        "\n",
        "SVM also gives good accuracy but it over fits the data.\n",
        "\n",
        "For sentiment analysis logistic regression and SVM are the two most appropriate models with accuracy of nearly 87%. "
      ],
      "metadata": {
        "id": "SP-j4_RDrKML"
      }
    }
  ]
}